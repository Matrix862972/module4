{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f36263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing TensorFlow import...\n",
      "âœ… TensorFlow imported successfully!\n",
      "âœ… TensorFlow version: 2.19.0\n",
      "Testing YOLO utilities...\n",
      "âœ… YOLO utilities imported successfully!\n",
      "âœ… Your environment is now fixed and ready to use!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "# Test TensorFlow import\n",
    "print(\"Testing TensorFlow import...\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(f\"âœ… TensorFlow imported successfully!\")\n",
    "print(f\"âœ… TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Test YOLO utilities\n",
    "print(\"Testing YOLO utilities...\")\n",
    "from yad2k.models.keras_yolo import yolo_head\n",
    "from yad2k.utils.utils import draw_boxes, get_colors_for_classes, scale_boxes, read_classes, read_anchors, preprocess_image\n",
    "\n",
    "print(\"âœ… YOLO utilities imported successfully!\")\n",
    "print(\"âœ… Your environment is now fixed and ready to use!\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31eef6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_filter_boxes(boxes,box_confidence,box_class_probs,threshold=0.6):\n",
    "    \"\"\"Filters YOLO boxes by thresholding on object and class confidence.\n",
    "    \n",
    "    Arguments:\n",
    "        boxes -- tensor of shape (19, 19, 5, 4)\n",
    "        box_confidence -- tensor of shape (19, 19, 5, 1)\n",
    "        box_class_probs -- tensor of shape (19, 19, 5, 80)\n",
    "        threshold -- real value, if [ highest class probability score < threshold],\n",
    "                     then get rid of the corresponding box\n",
    "\n",
    "    Returns:\n",
    "        scores -- tensor of shape (None,), containing the class probability score for selected boxes\n",
    "        boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes\n",
    "        classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes\n",
    "\n",
    "    Note: \"None\" is here because you don't know the exact number of selected boxes, as it depends on the threshold. \n",
    "    For example, the actual output size of scores would be (10,) if there are 10 boxes.\n",
    "    \"\"\"\n",
    "    x = 10\n",
    "    y = tf.constant(100)\n",
    "\n",
    "    # Step 1: Compute box scores\n",
    "    box_scores = box_class_probs * box_confidence\n",
    "    \n",
    "    # Step 2: Get best class for each box\n",
    "    # Pick the class index (0 to 79) with the highest score for each box.\n",
    "    box_classes = tf.math.argmax(box_scores, axis=-1)\n",
    "    # Get the actual score value (not index) of the best class for each box.\n",
    "    box_class_scores = tf.math.reduce_max(box_scores, axis=-1)\n",
    "\n",
    "    # Step 3: Create a mask for filtering\n",
    "    filtering_mask = (box_class_scores >= threshold)\n",
    "\n",
    "    # Step 4: Apply the mask (keep only good boxes)\n",
    "    # Keep only scores of good boxes\n",
    "    scores = tf.boolean_mask(box_class_scores, filtering_mask)\n",
    "    \n",
    "    # Keep only coordinates of good boxes\n",
    "    boxes = tf.boolean_mask(boxes, filtering_mask)\n",
    "    \n",
    "    # Keep only class indices of good boxes\n",
    "    classes = tf.boolean_mask(box_classes, filtering_mask)\n",
    "\n",
    "    # Return the final filtered results\n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22cf306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores[2] = 9.270486\n",
      "boxes[2] = [ 4.6399336  3.2303846  4.431282  -2.202031 ]\n",
      "classes[2] = 8\n",
      "scores.shape = (1789,)\n",
      "boxes.shape = (1789, 4)\n",
      "classes.shape = (1789,)\n",
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(10)\n",
    "box_confidence = tf.random.normal([19, 19, 5, 1], mean=1, stddev=4, seed = 1)\n",
    "boxes = tf.random.normal([19, 19, 5, 4], mean=1, stddev=4, seed = 1)\n",
    "box_class_probs = tf.random.normal([19, 19, 5, 80], mean=1, stddev=4, seed = 1)\n",
    "scores, boxes, classes = yolo_filter_boxes(boxes, box_confidence, box_class_probs, threshold = 0.5)\n",
    "print(\"scores[2] = \" + str(scores[2].numpy()))\n",
    "print(\"boxes[2] = \" + str(boxes[2].numpy()))\n",
    "print(\"classes[2] = \" + str(classes[2].numpy()))\n",
    "print(\"scores.shape = \" + str(scores.shape))\n",
    "print(\"boxes.shape = \" + str(boxes.shape))\n",
    "print(\"classes.shape = \" + str(classes.shape))\n",
    "\n",
    "assert type(scores) == EagerTensor, \"Use tensorflow functions\"\n",
    "assert type(boxes) == EagerTensor, \"Use tensorflow functions\"\n",
    "assert type(classes) == EagerTensor, \"Use tensorflow functions\"\n",
    "\n",
    "assert scores.shape == (1789,), \"Wrong shape in scores\"\n",
    "assert boxes.shape == (1789, 4), \"Wrong shape in boxes\"\n",
    "assert classes.shape == (1789,), \"Wrong shape in classes\"\n",
    "\n",
    "assert np.isclose(scores[2].numpy(), 9.270486), \"Values are wrong on scores\"\n",
    "assert np.allclose(boxes[2].numpy(), [4.6399336, 3.2303846, 4.431282, -2.202031]), \"Values are wrong on boxes\"\n",
    "assert classes[2].numpy() == 8, \"Values are wrong on classes\"\n",
    "\n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd1faf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    " \"\"\"\n",
    " Implement the intersection over union (IoU) between box1 and box2\n",
    "    \n",
    "    Arguments:\n",
    "    box1 -- first box, list object with coordinates (box1_x1, box1_y1, box1_x2, box_1_y2)\n",
    "    box2 -- second box, list object with coordinates (box2_x1, box2_y1, box2_x2, box2_y2)\n",
    " \"\"\"\n",
    "# Unpack the box coordinates\n",
    "# Extract each coordinate from the lists to make code cleaner.\n",
    "\n",
    " (box1_x1, box1_y1, box1_x2, box1_y2) = box1\n",
    " (box2_x1, box2_y1, box2_x2, box2_y2) = box2\n",
    "\n",
    "# Step 1: Find the Area intersection box\n",
    "\n",
    " xi1 = max(box1_x1, box2_x1)  # left of the overlapping area\n",
    " yi1 = max(box1_y1, box2_y1)  # top of the overlapping area\n",
    " xi2 = min(box1_x2, box2_x2)  # right of the overlapping area\n",
    " yi2 = min(box1_y2, box2_y2)  # bottom of the overlapping area\n",
    "\n",
    "#These lines calculate the area where the two boxes overlap.\n",
    "#max(0, ...) prevents negative area (if boxes donâ€™t overlap at all).\n",
    "\n",
    " inter_width = max(0, yi2 - yi1)   # height of overlap\n",
    " inter_height = max(0, xi2 - xi1)  # width of overlap\n",
    " inter_area = inter_width * inter_height\n",
    "\n",
    "# Step 2: Find the union area\n",
    " box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
    " box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
    " union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "# Step 3: Calculate the IoU\n",
    " iou = inter_area / union_area\n",
    "\n",
    " return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c576b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou for intersecting boxes = 0.14285714285714285\n",
      "iou for non-intersecting boxes = 0.0\n",
      "iou for boxes that only touch at vertices = 0.0\n",
      "iou for boxes that only touch at edges = 0.0\n",
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "## Test case 1: boxes intersect\n",
    "box1 = (2, 1, 4, 3)\n",
    "box2 = (1, 2, 3, 4)\n",
    "\n",
    "print(\"iou for intersecting boxes = \" + str(iou(box1, box2)))\n",
    "assert iou(box1, box2) < 1, \"The intersection area must be always smaller or equal than the union area.\"\n",
    "assert np.isclose(iou(box1, box2), 0.14285714), \"Wrong value. Check your implementation. Problem with intersecting boxes\"\n",
    "\n",
    "## Test case 2: boxes do not intersect\n",
    "box1 = (1,2,3,4)\n",
    "box2 = (5,6,7,8)\n",
    "print(\"iou for non-intersecting boxes = \" + str(iou(box1,box2)))\n",
    "assert iou(box1, box2) == 0, \"Intersection must be 0\"\n",
    "\n",
    "## Test case 3: boxes intersect at vertices only\n",
    "box1 = (1,1,2,2)\n",
    "box2 = (2,2,3,3)\n",
    "print(\"iou for boxes that only touch at vertices = \" + str(iou(box1,box2)))\n",
    "assert iou(box1, box2) == 0, \"Intersection at vertices must be 0\"\n",
    "\n",
    "## Test case 4: boxes intersect at edge only\n",
    "box1 = (1,1,3,3)\n",
    "box2 = (2,3,3,4)\n",
    "print(\"iou for boxes that only touch at edges = \" + str(iou(box1,box2)))\n",
    "assert iou(box1, box2) == 0, \"Intersection at edges must be 0\"\n",
    "\n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75effc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
    " \"\"\"\n",
    "    Applies Non-max suppression (NMS) to set of boxes\n",
    "    \n",
    "    Arguments:\n",
    "    scores -- tensor of shape (None,), output of yolo_filter_boxes()\n",
    "    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later)\n",
    "    classes -- tensor of shape (None,), output of yolo_filter_boxes()\n",
    "    max_boxes -- integer, maximum number of predicted boxes you'd like\n",
    "    iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering\n",
    "    \n",
    "    Returns:\n",
    "    scores -- tensor of shape (, None), predicted score for each box\n",
    "    boxes -- tensor of shape (4, None), predicted box coordinates\n",
    "    classes -- tensor of shape (, None), predicted class for each box\n",
    "    \n",
    "    Note: The \"None\" dimension of the output tensors has obviously to be less than max_boxes. Note also that this\n",
    "    function will transpose the shapes of scores, boxes, classes. This is made for convenience.\n",
    " \"\"\"\n",
    " # Step 1: Prepare Tensor for NMS\n",
    " # TensorFlow needs the max box number as a tf.Variable, not plain int\n",
    " max_boxes_tensor = tf.Variable(max_boxes, dtype='int32')\n",
    "\n",
    " # Step 2: Apply Non-Max Suppression\n",
    " # This function looks at boxes with high scores\n",
    " # Keeps the best one in overlapping areas\n",
    " # Removes the rest if they overlap too much (IoU â‰¥ iou_threshold)\n",
    " # Returns nms_indices: indexes of boxes you should keep\n",
    " nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes_tensor, iou_threshold)\n",
    "\n",
    "# Step 3: Select Only Best Boxes\n",
    "# tf.gather() picks values at the chosen indices\n",
    " scores = tf.gather(scores, nms_indices)\n",
    " boxes = tf.gather(boxes, nms_indices)\n",
    " classes = tf.gather(classes, nms_indices)\n",
    "\n",
    "# These are the final boxes, cleaned from duplicates and sorted by quality.\n",
    " return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b2318c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores[2] = 8.147684\n",
      "boxes[2] = [ 6.0797963   3.743308    1.3914018  -0.34089637]\n",
      "classes[2] = 1.7079165\n",
      "scores.shape = (10,)\n",
      "boxes.shape = (10, 4)\n",
      "classes.shape = (10,)\n",
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(10)\n",
    "scores = tf.random.normal([54,], mean=1, stddev=4, seed = 1)\n",
    "boxes = tf.random.normal([54, 4], mean=1, stddev=4, seed = 1)\n",
    "classes = tf.random.normal([54,], mean=1, stddev=4, seed = 1)\n",
    "scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes)\n",
    "\n",
    "assert type(scores) == EagerTensor, \"Use tensoflow functions\"\n",
    "print(\"scores[2] = \" + str(scores[2].numpy()))\n",
    "print(\"boxes[2] = \" + str(boxes[2].numpy()))\n",
    "print(\"classes[2] = \" + str(classes[2].numpy()))\n",
    "print(\"scores.shape = \" + str(scores.numpy().shape))\n",
    "print(\"boxes.shape = \" + str(boxes.numpy().shape))\n",
    "print(\"classes.shape = \" + str(classes.numpy().shape))\n",
    "\n",
    "assert type(scores) == EagerTensor, \"Use tensoflow functions\"\n",
    "assert type(boxes) == EagerTensor, \"Use tensoflow functions\"\n",
    "assert type(classes) == EagerTensor, \"Use tensoflow functions\"\n",
    "\n",
    "assert scores.shape == (10,), \"Wrong shape\"\n",
    "assert boxes.shape == (10, 4), \"Wrong shape\"\n",
    "assert classes.shape == (10,), \"Wrong shape\"\n",
    "\n",
    "assert np.isclose(scores[2].numpy(), 8.147684), \"Wrong value on scores\"\n",
    "assert np.allclose(boxes[2].numpy(), [ 6.0797963, 3.743308, 1.3914018, -0.34089637]), \"Wrong value on boxes\"\n",
    "assert np.isclose(classes[2].numpy(), 1.7079165), \"Wrong value on classes\"\n",
    "\n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b33d2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_boxes_to_corners(box_xy, box_wh):\n",
    " \"\"\"\n",
    " YOLO gives boxes using:\n",
    "\n",
    "box_xy: center of the box â†’ (x, y)\n",
    "\n",
    "box_wh: width and height of the box â†’ (w, h)\n",
    "\n",
    "But for drawing or calculating overlap, we need corners:\n",
    "\n",
    "Top-left: (x_min, y_min)\n",
    "\n",
    "Bottom-right: (x_max, y_max)\n",
    "\n",
    "This function converts center format to corner format\n",
    " \"\"\"\n",
    " # Step 1: Calculate box corners\n",
    " # You go half the width/height left and right from the center to get edges.\n",
    " box_mins = box_xy - (box_wh / 2.)\n",
    " box_maxes = box_xy + (box_wh / 2.)\n",
    " \n",
    " # Step 2: Reorder values to [y_min, x_min, y_max, x_max]\n",
    " return tf.keras.backend.concatenate([\n",
    "    box_mins[..., 1:2],  # y_min\n",
    "    box_mins[..., 0:1],  # x_min\n",
    "    box_maxes[..., 1:2],  # y_max\n",
    "    box_maxes[..., 0:1]  # x_max\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83deeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_eval(yolo_outputs, image_shape = (720, 1280), max_boxes=10, score_threshold=.6, iou_threshold=.5):\n",
    "  \"\"\"\n",
    "    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes.\n",
    "    \n",
    "    Arguments:\n",
    "    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors:\n",
    "                    box_xy: tensor of shape (None, 19, 19, 5, 2)\n",
    "                    box_wh: tensor of shape (None, 19, 19, 5, 2)\n",
    "                    box_confidence: tensor of shape (None, 19, 19, 5, 1)\n",
    "                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)\n",
    "    image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype)\n",
    "    max_boxes -- integer, maximum number of predicted boxes you'd like\n",
    "    score_threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box\n",
    "    iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering\n",
    "    \n",
    "    Returns:\n",
    "    scores -- tensor of shape (None, ), predicted score for each box\n",
    "    boxes -- tensor of shape (None, 4), predicted box coordinates\n",
    "    classes -- tensor of shape (None,), predicted class for each box\n",
    "  \"\"\"\n",
    "\n",
    "  # These are 4 tensors YOLO gives:\n",
    "  # box_xy: center of each box (x, y)\n",
    "  # box_wh: width and height of each box\n",
    "  # box_confidence: how sure YOLO is that thereâ€™s an object\n",
    "  # box_class_probs: how sure YOLO is about the object type (80 classes)\n",
    "  box_xy, box_wh, box_confidence, box_class_probs = yolo_outputs\n",
    "\n",
    "# Step 2: Convert to corner format\n",
    "# YOLO gives center points + width/height.\n",
    "# We convert that to corners: [y_min, x_min, y_max, x_max] for each box (needed for drawing and overlap calculation).\n",
    "  boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
    "\n",
    "# Step 3: Filter boxes using score threshold\n",
    "# Removes boxes that donâ€™t have high enough confidence.\n",
    "# Keeps boxes where the highest class score is â‰¥ score_threshold (0.6)\n",
    "  scores, boxes, classes = yolo_filter_boxes(boxes, box_confidence, box_class_probs, score_threshold)\n",
    "\n",
    "# Step 4: Scale boxes to original image size\n",
    "# YOLO works on 608Ã—608 images.\n",
    "# But your real image might be 720Ã—1280.\n",
    "# This step resizes boxes to match the actual image size.\n",
    "  boxes = scale_boxes(boxes, image_shape)\n",
    "\n",
    "# Step 5: Remove overlapping boxes using Non-Max Suppression\n",
    "# Keeps only the best boxes in overlapping areas.\n",
    "# max_boxes: max number of final boxes to return\n",
    "# iou_threshold: overlap limit (if too much overlap â†’ discard)\n",
    "  scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold)\n",
    "\n",
    "  return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2290df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores[2] = 171.60194\n",
      "boxes[2] = [-1240.3483 -3212.5881  -645.78    2024.3052]\n",
      "classes[2] = 16\n",
      "scores.shape = (10,)\n",
      "boxes.shape = (10, 4)\n",
      "classes.shape = (10,)\n",
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(10)\n",
    "yolo_outputs = (tf.random.normal([19, 19, 5, 2], mean=1, stddev=4, seed = 1),\n",
    "                tf.random.normal([19, 19, 5, 2], mean=1, stddev=4, seed = 1),\n",
    "                tf.random.normal([19, 19, 5, 1], mean=1, stddev=4, seed = 1),\n",
    "                tf.random.normal([19, 19, 5, 80], mean=1, stddev=4, seed = 1))\n",
    "scores, boxes, classes = yolo_eval(yolo_outputs)\n",
    "print(\"scores[2] = \" + str(scores[2].numpy()))\n",
    "print(\"boxes[2] = \" + str(boxes[2].numpy()))\n",
    "print(\"classes[2] = \" + str(classes[2].numpy()))\n",
    "print(\"scores.shape = \" + str(scores.numpy().shape))\n",
    "print(\"boxes.shape = \" + str(boxes.numpy().shape))\n",
    "print(\"classes.shape = \" + str(classes.numpy().shape))\n",
    "\n",
    "assert type(scores) == EagerTensor, \"Use tensoflow functions\"\n",
    "assert type(boxes) == EagerTensor, \"Use tensoflow functions\"\n",
    "assert type(classes) == EagerTensor, \"Use tensoflow functions\"\n",
    "\n",
    "assert scores.shape == (10,), \"Wrong shape\"\n",
    "assert boxes.shape == (10, 4), \"Wrong shape\"\n",
    "assert classes.shape == (10,), \"Wrong shape\"\n",
    "    \n",
    "assert np.isclose(scores[2].numpy(), 171.60194), \"Wrong value on scores\"\n",
    "assert np.allclose(boxes[2].numpy(), [-1240.3483, -3212.5881, -645.78, 2024.3052]), \"Wrong value on boxes\"\n",
    "assert np.isclose(classes[2].numpy(), 16), \"Wrong value on classes\"\n",
    "    \n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ac5a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = read_classes(\"model_data/coco_classes.txt\")\n",
    "anchors = read_anchors(\"model_data/yolo_anchors.txt\")\n",
    "model_image_size = (608, 608) # Same as yolo_model input layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e905fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YOLO Model Loading ===\n",
      "âŒ No yolo.h5 file found\n",
      "Attempting SavedModel with TFSMLayer...\n",
      "âŒ TFSMLayer failed: NewRandomAccessFile failed to Create/Open: model_data\\variables\\variables.data-00000-of-00001 : The system cannot find the file specified.\n",
      "; No such file or directory\n",
      "Attempting direct SavedModel load...\n",
      "âŒ TFSMLayer failed: NewRandomAccessFile failed to Create/Open: model_data\\variables\\variables.data-00000-of-00001 : The system cannot find the file specified.\n",
      "; No such file or directory\n",
      "Attempting direct SavedModel load...\n",
      "âŒ Direct SavedModel load failed: NewRandomAccessFile failed to Create/Open: model_data\\variables\\variables.data-00000-of-00001 : The system cannot find the file specified.\n",
      "; No such file or directory\n",
      "\n",
      "ğŸ”§ Creating placeholder model...\n",
      "ğŸ“‹ Model files appear to be incomplete due to Git LFS issues\n",
      "âœ… Placeholder model created\n",
      "\n",
      "ğŸ“– How to fix the model files:\n",
      "1. Install Git LFS: git lfs install\n",
      "2. Download actual files: git lfs pull\n",
      "3. Or download YOLO weights manually from the original source\n",
      "\n",
      "ğŸ’¡ Your YOLO algorithm functions still work perfectly!\n",
      "   The model file is only needed for inference on real images.\n",
      "\n",
      "âœ… Model loading process completed!\n",
      "âŒ Direct SavedModel load failed: NewRandomAccessFile failed to Create/Open: model_data\\variables\\variables.data-00000-of-00001 : The system cannot find the file specified.\n",
      "; No such file or directory\n",
      "\n",
      "ğŸ”§ Creating placeholder model...\n",
      "ğŸ“‹ Model files appear to be incomplete due to Git LFS issues\n",
      "âœ… Placeholder model created\n",
      "\n",
      "ğŸ“– How to fix the model files:\n",
      "1. Install Git LFS: git lfs install\n",
      "2. Download actual files: git lfs pull\n",
      "3. Or download YOLO weights manually from the original source\n",
      "\n",
      "ğŸ’¡ Your YOLO algorithm functions still work perfectly!\n",
      "   The model file is only needed for inference on real images.\n",
      "\n",
      "âœ… Model loading process completed!\n"
     ]
    }
   ],
   "source": [
    "# Load YOLO model - comprehensive approach for different file formats\n",
    "print(\"=== YOLO Model Loading ===\")\n",
    "\n",
    "# First, check if we have a working H5 file\n",
    "import os\n",
    "h5_file = \"model_data/yolo.h5\"\n",
    "savedmodel_dir = \"model_data\"\n",
    "\n",
    "if os.path.exists(h5_file):\n",
    "    try:\n",
    "        print(\"Found yolo.h5 file, attempting to load...\")\n",
    "        yolo_model = load_model(h5_file, compile=False)\n",
    "        print(\"âœ… Successfully loaded yolo.h5 model!\")\n",
    "        model_loaded = True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ H5 file is corrupted: {e}\")\n",
    "        model_loaded = False\n",
    "else:\n",
    "    print(\"âŒ No yolo.h5 file found\")\n",
    "    model_loaded = False\n",
    "\n",
    "# If H5 didn't work, try SavedModel approaches\n",
    "if not model_loaded:\n",
    "    try:\n",
    "        print(\"Attempting SavedModel with TFSMLayer...\")\n",
    "        yolo_model = tf.keras.layers.TFSMLayer(savedmodel_dir, call_endpoint='serving_default')\n",
    "        print(\"âœ… Successfully loaded SavedModel with TFSMLayer!\")\n",
    "        model_loaded = True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ TFSMLayer failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Attempting direct SavedModel load...\")\n",
    "            yolo_model = tf.saved_model.load(savedmodel_dir)\n",
    "            print(\"âœ… Successfully loaded SavedModel directly!\")\n",
    "            model_loaded = True\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ Direct SavedModel load failed: {e2}\")\n",
    "            model_loaded = False\n",
    "\n",
    "# If nothing worked, create a placeholder and explain the issue\n",
    "if not model_loaded:\n",
    "    print(\"\\nğŸ”§ Creating placeholder model...\")\n",
    "    print(\"ğŸ“‹ Model files appear to be incomplete due to Git LFS issues\")\n",
    "    \n",
    "    # Create a simple placeholder\n",
    "    input_layer = tf.keras.Input(shape=(608, 608, 3))\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(input_layer)\n",
    "    output_layer = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    yolo_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    print(\"âœ… Placeholder model created\")\n",
    "    print(\"\\nğŸ“– How to fix the model files:\")\n",
    "    print(\"1. Install Git LFS: git lfs install\")\n",
    "    print(\"2. Download actual files: git lfs pull\")\n",
    "    print(\"3. Or download YOLO weights manually from the original source\")\n",
    "    print(\"\\nğŸ’¡ Your YOLO algorithm functions still work perfectly!\")\n",
    "    print(\"   The model file is only needed for inference on real images.\")\n",
    "\n",
    "print(\"\\nâœ… Model loading process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "425c80b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YOLO Lab Implementation Summary ===\n",
      "\n",
      "âœ… WORKING COMPONENTS:\n",
      "   â€¢ TensorFlow and Keras properly installed\n",
      "   â€¢ All YOLO utility functions imported\n",
      "   â€¢ yolo_filter_boxes() - Filters boxes by confidence\n",
      "   â€¢ iou() - Calculates Intersection over Union\n",
      "   â€¢ yolo_non_max_suppression() - Removes duplicate detections\n",
      "   â€¢ yolo_boxes_to_corners() - Converts box formats\n",
      "   â€¢ yolo_eval() - Complete YOLO evaluation pipeline\n",
      "\n",
      "ğŸ”§ PLACEHOLDER CREATED:\n",
      "   â€¢ Model structure exists (for testing/learning)\n",
      "   â€¢ All your algorithm functions work correctly\n",
      "\n",
      "ğŸ“‹ WHAT YOU CAN DO NOW:\n",
      "   1. Test all your YOLO functions with synthetic data\n",
      "   2. Run the test cells to verify implementations\n",
      "   3. Study the YOLO algorithm concepts\n",
      "   4. Practice with the evaluation pipeline\n",
      "\n",
      "ğŸš€ TO USE WITH REAL IMAGES:\n",
      "   â€¢ Fix the Git LFS issue to get actual model weights\n",
      "   â€¢ Or download pre-trained YOLO weights manually\n",
      "   â€¢ Your algorithm implementations are ready!\n",
      "\n",
      "âœ¨ Great job implementing the YOLO algorithm! âœ¨\n",
      "\n",
      "=== Quick Test ===\n",
      "âœ… Model accepts input shape: (1, 608, 608, 3)\n",
      "âœ… Model produces output shape: (1, 1)\n",
      "âœ… Basic model structure is working!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ‰ Your YOLO Implementation Status\n",
    "\n",
    "print(\"=== YOLO Lab Implementation Summary ===\")\n",
    "print()\n",
    "print(\"âœ… WORKING COMPONENTS:\")\n",
    "print(\"   â€¢ TensorFlow and Keras properly installed\")\n",
    "print(\"   â€¢ All YOLO utility functions imported\")\n",
    "print(\"   â€¢ yolo_filter_boxes() - Filters boxes by confidence\")\n",
    "print(\"   â€¢ iou() - Calculates Intersection over Union\")\n",
    "print(\"   â€¢ yolo_non_max_suppression() - Removes duplicate detections\")\n",
    "print(\"   â€¢ yolo_boxes_to_corners() - Converts box formats\")\n",
    "print(\"   â€¢ yolo_eval() - Complete YOLO evaluation pipeline\")\n",
    "print()\n",
    "print(\"ğŸ”§ PLACEHOLDER CREATED:\")\n",
    "print(\"   â€¢ Model structure exists (for testing/learning)\")\n",
    "print(\"   â€¢ All your algorithm functions work correctly\")\n",
    "print()\n",
    "print(\"ğŸ“‹ WHAT YOU CAN DO NOW:\")\n",
    "print(\"   1. Test all your YOLO functions with synthetic data\")\n",
    "print(\"   2. Run the test cells to verify implementations\")\n",
    "print(\"   3. Study the YOLO algorithm concepts\")\n",
    "print(\"   4. Practice with the evaluation pipeline\")\n",
    "print()\n",
    "print(\"ğŸš€ TO USE WITH REAL IMAGES:\")\n",
    "print(\"   â€¢ Fix the Git LFS issue to get actual model weights\")\n",
    "print(\"   â€¢ Or download pre-trained YOLO weights manually\")\n",
    "print(\"   â€¢ Your algorithm implementations are ready!\")\n",
    "print()\n",
    "print(\"âœ¨ Great job implementing the YOLO algorithm! âœ¨\")\n",
    "\n",
    "# Test if the basic structure works\n",
    "print(\"\\n=== Quick Test ===\")\n",
    "try:\n",
    "    # Test with dummy data\n",
    "    dummy_input = tf.random.normal([1, 608, 608, 3])\n",
    "    output = yolo_model(dummy_input)\n",
    "    print(f\"âœ… Model accepts input shape: {dummy_input.shape}\")\n",
    "    print(f\"âœ… Model produces output shape: {output.shape}\")\n",
    "    print(\"âœ… Basic model structure is working!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Model test failed: {e}\")\n",
    "    print(\"This is expected with the placeholder model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80f8f6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YOLO Model Architecture Summary ===\n",
      "\n",
      "ğŸ” Model Type Analysis:\n",
      "   â€¢ Model class: <class 'keras.src.models.functional.Functional'>\n",
      "   â€¢ Model name: functional_2\n",
      "\n",
      "ğŸ“‹ Model Summary:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_2      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m608\u001b[0m, \u001b[38;5;34m608\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_2      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚             \u001b[38;5;34m4\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“ Input/Output Information:\n",
      "   â€¢ Input shape: (None, 608, 608, 3)\n",
      "   â€¢ Output shape: (None, 1)\n",
      "   â€¢ Actual input shape: (1, 608, 608, 3)\n",
      "   â€¢ Actual output shape: (1, 1)\n",
      "\n",
      "ğŸ”¢ Model Parameters:\n",
      "   â€¢ Total parameters: 4\n",
      "   â€¢ Trainable parameters: 4\n",
      "   â€¢ Non-trainable parameters: 0\n",
      "\n",
      "ğŸ—ï¸ Layer Information:\n",
      "   â€¢ Number of layers: 3\n",
      "   â€¢ Layer types:\n",
      "     1. input_layer_2: InputLayer\n",
      "     2. global_average_pooling2d_2: GlobalAveragePooling2D\n",
      "     3. dense_2: Dense\n",
      "\n",
      "ğŸ’¡ Note about YOLO Architecture:\n",
      "   â€¢ A real YOLO model typically has:\n",
      "     - Convolutional backbone (like Darknet)\n",
      "     - Feature pyramid network\n",
      "     - Detection heads for different scales\n",
      "     - Outputs: box coordinates, confidence, class probabilities\n",
      "   â€¢ Input: RGB images (608Ã—608Ã—3)\n",
      "   â€¢ Output: Detections for multiple object classes\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š YOLO Model Summary\n",
    "\n",
    "print(\"=== YOLO Model Architecture Summary ===\")\n",
    "print()\n",
    "\n",
    "# Check what type of model we have\n",
    "print(\"ğŸ” Model Type Analysis:\")\n",
    "print(f\"   â€¢ Model class: {type(yolo_model)}\")\n",
    "print(f\"   â€¢ Model name: {getattr(yolo_model, 'name', 'Not available')}\")\n",
    "print()\n",
    "\n",
    "# Try to get model summary\n",
    "try:\n",
    "    print(\"ğŸ“‹ Model Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    yolo_model.summary()\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not display summary: {e}\")\n",
    "    print(\"This might be due to the placeholder model structure\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Display input/output shapes\n",
    "try:\n",
    "    print(\"ğŸ“ Input/Output Information:\")\n",
    "    if hasattr(yolo_model, 'input_shape'):\n",
    "        print(f\"   â€¢ Input shape: {yolo_model.input_shape}\")\n",
    "    if hasattr(yolo_model, 'output_shape'):\n",
    "        print(f\"   â€¢ Output shape: {yolo_model.output_shape}\")\n",
    "    \n",
    "    # Test with dummy input to see actual shapes\n",
    "    dummy_input = tf.random.normal([1, 608, 608, 3])\n",
    "    dummy_output = yolo_model(dummy_input)\n",
    "    print(f\"   â€¢ Actual input shape: {dummy_input.shape}\")\n",
    "    print(f\"   â€¢ Actual output shape: {dummy_output.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not get shape information: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Model parameters count\n",
    "try:\n",
    "    print(\"ğŸ”¢ Model Parameters:\")\n",
    "    if hasattr(yolo_model, 'count_params'):\n",
    "        total_params = yolo_model.count_params()\n",
    "        print(f\"   â€¢ Total parameters: {total_params:,}\")\n",
    "        \n",
    "        # Try to get trainable/non-trainable split\n",
    "        try:\n",
    "            trainable_params = sum([tf.keras.backend.count_params(w) for w in yolo_model.trainable_weights])\n",
    "            non_trainable_params = sum([tf.keras.backend.count_params(w) for w in yolo_model.non_trainable_weights])\n",
    "            print(f\"   â€¢ Trainable parameters: {trainable_params:,}\")\n",
    "            print(f\"   â€¢ Non-trainable parameters: {non_trainable_params:,}\")\n",
    "        except:\n",
    "            print(\"   â€¢ Parameter breakdown not available\")\n",
    "    else:\n",
    "        print(\"   â€¢ Parameter count not available for this model type\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not count parameters: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Layer information\n",
    "try:\n",
    "    print(\"ğŸ—ï¸ Layer Information:\")\n",
    "    if hasattr(yolo_model, 'layers'):\n",
    "        print(f\"   â€¢ Number of layers: {len(yolo_model.layers)}\")\n",
    "        print(\"   â€¢ Layer types:\")\n",
    "        for i, layer in enumerate(yolo_model.layers[:10]):  # Show first 10 layers\n",
    "            print(f\"     {i+1}. {layer.name}: {type(layer).__name__}\")\n",
    "        if len(yolo_model.layers) > 10:\n",
    "            print(f\"     ... and {len(yolo_model.layers) - 10} more layers\")\n",
    "    else:\n",
    "        print(\"   â€¢ Layer information not available\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not get layer information: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# For actual YOLO model, this would show the architecture\n",
    "print(\"ğŸ’¡ Note about YOLO Architecture:\")\n",
    "print(\"   â€¢ A real YOLO model typically has:\")\n",
    "print(\"     - Convolutional backbone (like Darknet)\")\n",
    "print(\"     - Feature pyramid network\")\n",
    "print(\"     - Detection heads for different scales\")\n",
    "print(\"     - Outputs: box coordinates, confidence, class probabilities\")\n",
    "print(\"   â€¢ Input: RGB images (608Ã—608Ã—3)\")\n",
    "print(\"   â€¢ Output: Detections for multiple object classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8a1cad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== What a Real YOLO Model Would Look Like ===\n",
      "\n",
      "ğŸ—ï¸ Typical YOLO Architecture:\n",
      "   1. INPUT: 608Ã—608Ã—3 RGB image\n",
      "   2. BACKBONE: Darknet-53 (or similar CNN)\n",
      "      â€¢ 53 convolutional layers\n",
      "      â€¢ Residual connections\n",
      "      â€¢ Feature extraction at multiple scales\n",
      "   3. NECK: Feature Pyramid Network (FPN)\n",
      "      â€¢ Combines features from different scales\n",
      "      â€¢ Upsampling and concatenation\n",
      "   4. HEAD: Detection layers\n",
      "      â€¢ 3 different scales (13Ã—13, 26Ã—26, 52Ã—52)\n",
      "      â€¢ Each scale predicts: [x, y, w, h, confidence, class_probs]\n",
      "   5. OUTPUT: Object detections\n",
      "      â€¢ Bounding boxes\n",
      "      â€¢ Confidence scores\n",
      "      â€¢ Class probabilities\n",
      "\n",
      "ğŸ“Š Real YOLO Model Stats:\n",
      "   â€¢ Parameters: ~60+ million\n",
      "   â€¢ Layers: 100+ layers\n",
      "   â€¢ Model size: ~200+ MB\n",
      "   â€¢ Training: Pre-trained on COCO dataset\n",
      "   â€¢ Classes: 80 object categories\n",
      "\n",
      "ğŸ”— Current Placeholder vs Real Model:\n",
      "   Current placeholder:\n",
      "      â€¢ Parameters: 4\n",
      "      â€¢ Layers: 3\n",
      "      â€¢ Purpose: Structure testing only\n",
      "\n",
      "   Real YOLO model:\n",
      "      â€¢ Parameters: ~60,000,000+\n",
      "      â€¢ Layers: 100+\n",
      "      â€¢ Purpose: Actual object detection\n",
      "\n",
      "ğŸ’¡ To get the real model:\n",
      "   1. Fix Git LFS: git lfs install && git lfs pull\n",
      "   2. Or download from: https://pjreddie.com/media/files/yolov3.weights\n",
      "   3. Convert to Keras format if needed\n",
      "\n",
      "ğŸ› ï¸ Would you like a more realistic placeholder architecture?\n",
      "   (This would be for educational purposes - still not trained)\n",
      "âœ… Architecture explanation completed!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ Real YOLO Model Architecture Explanation\n",
    "\n",
    "print(\"=== What a Real YOLO Model Would Look Like ===\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ—ï¸ Typical YOLO Architecture:\")\n",
    "print(\"   1. INPUT: 608Ã—608Ã—3 RGB image\")\n",
    "print(\"   2. BACKBONE: Darknet-53 (or similar CNN)\")\n",
    "print(\"      â€¢ 53 convolutional layers\")\n",
    "print(\"      â€¢ Residual connections\")\n",
    "print(\"      â€¢ Feature extraction at multiple scales\")\n",
    "print(\"   3. NECK: Feature Pyramid Network (FPN)\")\n",
    "print(\"      â€¢ Combines features from different scales\")\n",
    "print(\"      â€¢ Upsampling and concatenation\")\n",
    "print(\"   4. HEAD: Detection layers\")\n",
    "print(\"      â€¢ 3 different scales (13Ã—13, 26Ã—26, 52Ã—52)\")\n",
    "print(\"      â€¢ Each scale predicts: [x, y, w, h, confidence, class_probs]\")\n",
    "print(\"   5. OUTPUT: Object detections\")\n",
    "print(\"      â€¢ Bounding boxes\")\n",
    "print(\"      â€¢ Confidence scores\")\n",
    "print(\"      â€¢ Class probabilities\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“Š Real YOLO Model Stats:\")\n",
    "print(\"   â€¢ Parameters: ~60+ million\")\n",
    "print(\"   â€¢ Layers: 100+ layers\")\n",
    "print(\"   â€¢ Model size: ~200+ MB\")\n",
    "print(\"   â€¢ Training: Pre-trained on COCO dataset\")\n",
    "print(\"   â€¢ Classes: 80 object categories\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ”— Current Placeholder vs Real Model:\")\n",
    "print(\"   Current placeholder:\")\n",
    "print(f\"      â€¢ Parameters: {yolo_model.count_params():,}\")\n",
    "print(f\"      â€¢ Layers: {len(yolo_model.layers)}\")\n",
    "print(f\"      â€¢ Purpose: Structure testing only\")\n",
    "print()\n",
    "print(\"   Real YOLO model:\")\n",
    "print(\"      â€¢ Parameters: ~60,000,000+\")\n",
    "print(\"      â€¢ Layers: 100+\")\n",
    "print(\"      â€¢ Purpose: Actual object detection\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ’¡ To get the real model:\")\n",
    "print(\"   1. Fix Git LFS: git lfs install && git lfs pull\")\n",
    "print(\"   2. Or download from: https://pjreddie.com/media/files/yolov3.weights\")\n",
    "print(\"   3. Convert to Keras format if needed\")\n",
    "print()\n",
    "\n",
    "# Optional: Create a more realistic placeholder architecture\n",
    "print(\"ğŸ› ï¸ Would you like a more realistic placeholder architecture?\")\n",
    "print(\"   (This would be for educational purposes - still not trained)\")\n",
    "\n",
    "def create_realistic_yolo_placeholder():\n",
    "    \"\"\"Create a more realistic YOLO architecture (untrained)\"\"\"\n",
    "    print(\"\\nğŸ—ï¸ Creating realistic YOLO placeholder...\")\n",
    "    \n",
    "    # Input\n",
    "    inputs = tf.keras.Input(shape=(608, 608, 3))\n",
    "    \n",
    "    # Simplified backbone (much smaller than real Darknet)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Detection head (simplified)\n",
    "    # Real YOLO would have multiple scales, we'll do one for simplicity\n",
    "    x = tf.keras.layers.Conv2D(1024, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, 1, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Output: 19x19 grid, 5 boxes per cell, 85 values per box (4 coords + 1 conf + 80 classes)\n",
    "    predictions = tf.keras.layers.Conv2D(5 * 85, 1, padding='same')(x)\n",
    "    \n",
    "    # Reshape to proper format\n",
    "    predictions = tf.keras.layers.Reshape((19, 19, 5, 85))(predictions)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# Uncomment the next line if you want to create the realistic placeholder\n",
    "# realistic_yolo = create_realistic_yolo_placeholder()\n",
    "# realistic_yolo.summary()\n",
    "\n",
    "print(\"âœ… Architecture explanation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa05c8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_2      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m608\u001b[0m, \u001b[38;5;34m608\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_2      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚             \u001b[38;5;34m4\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yolo_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "365110ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_file):\n",
    "    \"\"\"\n",
    "    Runs the graph to predict boxes for \"image_file\". Prints and plots the predictions.\n",
    "    \n",
    "    Arguments:\n",
    "    image_file -- name of an image stored in the \"images\" folder.\n",
    "    \n",
    "    Returns:\n",
    "    out_scores -- tensor of shape (None, ), scores of the predicted boxes\n",
    "    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes\n",
    "    out_classes -- tensor of shape (None, ), class index of the predicted boxes\n",
    "    \n",
    "    Note: \"None\" actually represents the number of predicted boxes, it varies between 0 and max_boxes. \n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess your image\n",
    "    image, image_data = preprocess_image(\"images/\" + image_file, model_image_size = (608, 608))\n",
    "    \n",
    "    yolo_model_outputs = yolo_model(image_data) # It's output is of shape (m, 19, 19, 5, 85) \n",
    "    # But yolo_eval takes input a tensor contains 4 tensors: box_xy,box_wh, box_confidence & box_class_probs\n",
    "    yolo_outputs = yolo_head(yolo_model_outputs, anchors, len(class_names))\n",
    "    \n",
    "    out_scores, out_boxes, out_classes = yolo_eval(yolo_outputs, [image.size[1],  image.size[0]], 10, 0.3, 0.5)\n",
    "\n",
    "    # Print predictions info\n",
    "    print('Found {} boxes for {}'.format(len(out_boxes), \"images/\" + image_file))\n",
    "    # Generate colors for drawing bounding boxes.\n",
    "    colors = get_colors_for_classes(len(class_names))\n",
    "    # Draw bounding boxes on the image file\n",
    "    #draw_boxes2(image, out_scores, out_boxes, out_classes, class_names, colors, image_shape)\n",
    "    draw_boxes(image, out_boxes, out_classes, class_names, out_scores)\n",
    "    # Save the predicted bounding box on the image\n",
    "    image.save(os.path.join(\"out\", str(image_file).split('.')[0]+\"_annotated.\" +str(image_file).split('.')[1] ), quality=100)\n",
    "    # Display the results in the notebook\n",
    "    output_image = Image.open(os.path.join(\"out\", str(image_file).split('.')[0]+\"_annotated.\" +str(image_file).split('.')[1] ))\n",
    "    imshow(output_image)\n",
    "\n",
    "    return out_scores, out_boxes, out_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdf38576",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m out_scores, out_boxes, out_classes = \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0001.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mpredict\u001b[39m\u001b[34m(image_file)\u001b[39m\n\u001b[32m     19\u001b[39m yolo_model_outputs = yolo_model(image_data) \u001b[38;5;66;03m# It's output is of shape (m, 19, 19, 5, 85) \u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# But yolo_eval takes input a tensor contains 4 tensors: box_xy,box_wh, box_confidence & box_class_probs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m yolo_outputs = \u001b[43myolo_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_model_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m out_scores, out_boxes, out_classes = yolo_eval(yolo_outputs, [image.size[\u001b[32m1\u001b[39m],  image.size[\u001b[32m0\u001b[39m]], \u001b[32m10\u001b[39m, \u001b[32m0.3\u001b[39m, \u001b[32m0.5\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Print predictions info\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Andrew Ng Deep Learning Course\\module 4\\lab\\week 3\\lab 1\\yad2k\\models\\keras_yolo.py:101\u001b[39m, in \u001b[36myolo_head\u001b[39m\u001b[34m(feats, anchors, num_classes)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# In YOLO the height index is the inner most iteration.\u001b[39;00m\n\u001b[32m    100\u001b[39m conv_height_index = K.arange(\u001b[32m0\u001b[39m, stop=conv_dims[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m conv_width_index = K.arange(\u001b[32m0\u001b[39m, stop=\u001b[43mconv_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    102\u001b[39m conv_height_index = K.tile(conv_height_index, [conv_dims[\u001b[32m1\u001b[39m]])\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# TODO: Repeat_elements and tf.split doesn't support dynamic splits.\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\framework\\ops.py:6006\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   6004\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   6005\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m6006\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "out_scores, out_boxes, out_classes = predict(\"0001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ffbd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
